{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0365f040",
   "metadata": {},
   "outputs": [],
   "source": [
    "### code snippet that demonstrates how to perform road lane line detection using OpenCV:\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load image and convert to grayscale\n",
    "img = cv2.imread('road.png')\n",
    "img = cv2.resize(img, (640, 480))\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply Canny edge detection\n",
    "edges = cv2.Canny(gray, 50, 150, apertureSize=3)\n",
    "\n",
    "# Define region of interest\n",
    "roi_vertices = np.array([[(0, img.shape[0]), (img.shape[1] // 2, img.shape[0] // 2), (img.shape[1], img.shape[0])]], dtype=np.int32)\n",
    "mask = np.zeros_like(edges)\n",
    "cv2.fillPoly(mask, roi_vertices, 255)\n",
    "masked_edges = cv2.bitwise_and(edges, mask)\n",
    "\n",
    "# Apply Hough Line Transform\n",
    "lines = cv2.HoughLinesP(masked_edges, rho=1, theta=np.pi/180, threshold=50, minLineLength=100, maxLineGap=50)\n",
    "\n",
    "# Draw detected lines on original image\n",
    "for line in lines:\n",
    "    x1, y1, x2, y2 = line[0]\n",
    "    cv2.line(img, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "\n",
    "# Display result\n",
    "cv2.imshow('result', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ed0990",
   "metadata": {},
   "outputs": [],
   "source": [
    "### perform road lane line detection on a video stream using OpenCV:\n",
    "## Download the video file by using the following link and running the code.\n",
    "\n",
    "## https://www.kaggle.com/datasets/dpamgautam/video-file-for-lane-detection-project\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define region of interest\n",
    "def roi_mask(img, vertices):\n",
    "    mask = np.zeros_like(img)\n",
    "    cv2.fillPoly(mask, vertices, 255)\n",
    "    masked_img = cv2.bitwise_and(img, mask)\n",
    "    return masked_img\n",
    "\n",
    "# Apply Hough Line Transform\n",
    "def hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):\n",
    "    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n",
    "    return lines\n",
    "\n",
    "# Draw detected lines on original image\n",
    "def draw_lines(img, lines, color=[255, 0, 0], thickness=5):\n",
    "    for line in lines:\n",
    "        for x1, y1, x2, y2 in line:\n",
    "            cv2.line(img, (x1, y1), (x2, y2), color, thickness)\n",
    "\n",
    "# Road Lane Line Detection on video stream\n",
    "cap = cv2.VideoCapture('road.mp4')\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        try:\n",
    "            # Convert to grayscale\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Apply Gaussian blur\n",
    "            blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "            # Apply Canny edge detection\n",
    "            edges = cv2.Canny(blur, 50, 150)\n",
    "\n",
    "            # Define region of interest\n",
    "            roi_vertices = np.array([[(0, frame.shape[0]), (frame.shape[1] // 2, frame.shape[0] // 2 + 50), (frame.shape[1], frame.shape[0])]], dtype=np.int32)\n",
    "            roi = roi_mask(edges, roi_vertices)\n",
    "\n",
    "            # Apply Hough Line Transform\n",
    "            lines = hough_lines(roi, rho=2, theta=np.pi/180, threshold=50, min_line_len=100, max_line_gap=50)\n",
    "\n",
    "            # Draw detected lines on original image\n",
    "            line_img = np.zeros((frame.shape[0], frame.shape[1], 3), dtype=np.uint8)\n",
    "            draw_lines(line_img, lines)\n",
    "\n",
    "            # Overlay detected lane lines on original image\n",
    "            result = cv2.addWeighted(frame, 0.8, line_img, 1, 0)\n",
    "\n",
    "            # Display result\n",
    "            cv2.imshow('result', result)\n",
    "\n",
    "            # Exit on 'q' key press\n",
    "            if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                break\n",
    "                \n",
    "        except: pass\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b282cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### detect corners with the Harris Corner Detector in OpenCV:\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load image\n",
    "img = cv2.imread('blox.jpg')\n",
    "\n",
    "# Convert image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply Harris Corner Detector\n",
    "dst = cv2.cornerHarris(gray, 2, 3, 0.04)\n",
    "\n",
    "# Threshold for an optimal value\n",
    "thresh = 0.01 * dst.max()\n",
    "\n",
    "# Create a black image to display the corners\n",
    "corner_img = np.zeros_like(img)\n",
    "\n",
    "# Draw detected corners on the black image\n",
    "for row_index in range(dst.shape[0]):\n",
    "    for column_index in range(dst.shape[1]):\n",
    "        if dst[row_index ,column_index] > thresh:\n",
    "            cv2.circle(img, (column_index,row_index), 3, (0,255,0), 1)\n",
    "\n",
    "# Display the original image and the detected corners\n",
    "cv2.imshow('Image', img)\n",
    "# cv2.imshow('Corners', corner_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c46b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "### detect corners with the Shi-Tomasi Corner Detector in OpenCV:\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load image\n",
    "img = cv2.imread('blox.jpg')\n",
    "\n",
    "# Convert image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Set parameters for Shi-Tomasi Corner Detector\n",
    "max_corners = 100\n",
    "quality_level = 0.3\n",
    "min_distance = 7\n",
    "block_size = 7\n",
    "\n",
    "# Apply Shi-Tomasi Corner Detector\n",
    "corners = cv2.goodFeaturesToTrack(gray, max_corners, quality_level, min_distance, blockSize=block_size)\n",
    "\n",
    "# Draw detected corners on the original image\n",
    "corners = np.int0(corners)\n",
    "for i in corners:\n",
    "    x, y = i.ravel()\n",
    "    cv2.circle(img, (x, y), 5, (0, 255, 0), -1)\n",
    "\n",
    "# Display the original image with detected corners\n",
    "cv2.imshow('Image', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec4edce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### detect corners with the FAST algorithm in OpenCV:\n",
    "\n",
    "import cv2\n",
    "\n",
    "# Load image\n",
    "img = cv2.imread('blox.jpg')\n",
    "\n",
    "# Convert image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Set parameters for FAST corner detection\n",
    "fast = cv2.FastFeatureDetector_create(threshold=25)\n",
    "\n",
    "# Detect corners using FAST algorithm\n",
    "kp = fast.detect(gray, None)\n",
    "\n",
    "# Draw detected corners on the original image\n",
    "img2 = cv2.drawKeypoints(img, kp, None, color=(0, 255, 0))\n",
    "\n",
    "# Display the original image with detected corners\n",
    "cv2.imshow('Image', img2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35e4c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "### corners with the blob detection algorithm in OpenCV:\n",
    "\n",
    "import cv2\n",
    "\n",
    "# Load image\n",
    "img = cv2.imread('blox.jpg')\n",
    "\n",
    "# Convert image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Set parameters for blob detection\n",
    "params = cv2.SimpleBlobDetector_Params()\n",
    "\n",
    "# Filter by color and size\n",
    "params.filterByColor = True\n",
    "params.blobColor = 255\n",
    "params.filterByArea = True\n",
    "params.minArea = 100\n",
    "\n",
    "# Create a blob detector object\n",
    "detector = cv2.SimpleBlobDetector_create(params)\n",
    "\n",
    "# Detect blobs using the blob detector\n",
    "keypoints = detector.detect(gray)\n",
    "\n",
    "# Draw detected blobs on the original image\n",
    "img_with_keypoints = cv2.drawKeypoints(img, keypoints, np.array([]), (0,0,255), cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "# Display the original image with detected blobs\n",
    "cv2.imshow('Image', img_with_keypoints)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e63ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### detect corners with the SIFT algorithm in OpenCV:\n",
    "\n",
    "\n",
    "import cv2\n",
    "\n",
    "# Load image\n",
    "img = cv2.imread('blox.jpg')\n",
    "\n",
    "# Convert image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create a SIFT detector object\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Detect keypoints using the SIFT detector\n",
    "keypoints = sift.detect(gray, None)\n",
    "\n",
    "# Draw detected keypoints on the original image\n",
    "img_with_keypoints = cv2.drawKeypoints(img, keypoints, None)\n",
    "\n",
    "# Display the original image with detected keypoints\n",
    "cv2.imshow('Image', img_with_keypoints)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3aac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FLANN can be a powerful tool for feature matching in OpenCV\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load images\n",
    "img1 = cv2.imread('dog.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread('dog-head.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "# Initiate SIFT detector\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Find keypoints and descriptors for both images\n",
    "kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "kp2, des2 = sift.detectAndCompute(img2, None)\n",
    "\n",
    "# FLANN parameters\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "search_params = dict(checks=50)\n",
    "\n",
    "# Create FLANN matcher\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "# Match descriptors\n",
    "matches = flann.knnMatch(des1, des2, k=2)\n",
    "\n",
    "# Ratio test as per Lowe's paper\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    \n",
    "\"\"\"\n",
    "m: This variable represents the first best match between a feature descriptor in des1 and the feature descriptors in des2.    \n",
    "n: This variable represents the second-best match between a feature descriptor in des1 and the feature descriptors in des2\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    if m.distance < 0.7 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "# Draw matches\n",
    "img3 = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "# Display image\n",
    "cv2.imshow('Matches', img3)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a42484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BackgroundSubtractorMOG2 method in OpenCV:\n",
    "\n",
    "import cv2\n",
    "\n",
    "# Create background subtraction object\n",
    "bg_sub = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "# Open video file or stream\n",
    "cap = cv2.VideoCapture('output.mp4')\n",
    "\n",
    "while True:\n",
    "    # Read frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        # Apply background subtraction\n",
    "        fg_mask = bg_sub.apply(frame)\n",
    "\n",
    "        # Apply morphological operations to clean up mask\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "        fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "        # Display result\n",
    "        cv2.imshow('Foreground Mask', fg_mask)\n",
    "        cv2.imshow('Original Frame', frame)\n",
    "\n",
    "        # Exit on ESC key\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else: break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158ac4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BackgroundSubtractorKNN method in OpenCV:\n",
    "\n",
    "\n",
    "\n",
    "import cv2\n",
    "\n",
    "# Create a video capture object\n",
    "cap = cv2.VideoCapture('output.mp4')\n",
    "\n",
    "# Create a BackgroundSubtractorKNN object\n",
    "bg_subtractor = cv2.createBackgroundSubtractorKNN()\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Apply the background subtraction\n",
    "    fg_mask = bg_subtractor.apply(frame)\n",
    "\n",
    "    # Display the original frame and the foreground mask\n",
    "    cv2.imshow('Original Frame', frame)\n",
    "    cv2.imshow('Foreground Mask', fg_mask)\n",
    "\n",
    "    # Check for user input to exit\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and destroy all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633986d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaeee7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0c0209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1703bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5677c65d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3d2665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5c98c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c5c394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96d33ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc536c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c09ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49bc25b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50144662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
