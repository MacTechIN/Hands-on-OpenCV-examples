{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54366f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### perform YOLO v3 object detection in OpenCV:\n",
    "### Download the “yolov3.weights”, and “yolov3.cfg” files from the following website. \n",
    "# https://pjreddie.com/darknet/yolo/\n",
    "### YOLO v3 object detection OpenCV code.\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "## load YOLO3 weights and cfg file\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\",\"yolov3.cfg\") \n",
    "\n",
    "## load the class values\n",
    "classes = []\n",
    "with open(\"coco.names\",\"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "#print(classes)\n",
    "\n",
    "## get the convolution layer\n",
    "layer_names = net.getLayerNames()\n",
    "outputlayers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    " \n",
    "colors= np.random.uniform(0,255,size=(len(classes),3))\n",
    " \n",
    "#loading image or video\n",
    "cap=cv2.VideoCapture(\"dog.mp4\") #0 for 1st webcam\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "# Get the width and height of the video frames\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "vids = cv2.VideoWriter('yolo.mp4', fourcc, 30, (width, height))\n",
    "\n",
    "while True:\n",
    "    ret,frame= cap.read() # read the frame\n",
    "    \n",
    "    if not ret: break\n",
    "    \n",
    "    height,width,channels = frame.shape\n",
    "    \n",
    "    #detecting objects, blob conversion which is basically extracting features from image\n",
    "    blob = cv2.dnn.blobFromImage(frame,0.00392,(320,320),(0,0,0),True,crop=False) #reduce 416 to 320         \n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(outputlayers)\n",
    "\n",
    "    #Showing info on screen/ get confidence score of algorithm in detecting an object in blob\n",
    "    class_ids=[]\n",
    "    confidences=[]\n",
    "    boxes=[]\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.3:\n",
    "                \n",
    "                #object detected\n",
    "                center_x= int(detection[0]*width)\n",
    "                center_y= int(detection[1]*height)\n",
    "                w = int(detection[2]*width)\n",
    "                h = int(detection[3]*height)\n",
    "                \n",
    "                #rectangle co-ordinaters\n",
    "                x=int(center_x - w/2)\n",
    "                y=int(center_y - h/2)\n",
    "\n",
    "                boxes.append([x,y,w,h]) #put all rectangle areas\n",
    "                confidences.append(float(confidence)) #how confidence was that object detected and show that percentage\n",
    "                class_ids.append(class_id) #name of the object tha was detected\n",
    "                \n",
    "                \n",
    "    # any box having value less than 0.6- that will be removed\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes,confidences,0.4,0.6)\n",
    "\n",
    "    # put the text values on the frame\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x,y,w,h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            confidence= confidences[i]\n",
    "            color = colors[class_ids[i]]\n",
    "            cv2.rectangle(frame,(x,y),(x+w,y+h),color,2)\n",
    "            cv2.putText(frame,label+\" \"+str(round(confidence,2)),(x,y+30),font,1,(255,255,255),2)\n",
    "      \n",
    "    #writing the frame \n",
    "    vids.write(frame) \n",
    "    #wait 1ms the loop will start again and we will process the next frame\n",
    "    cv2.imshow(\"Video\",frame)   \n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'): #Q key stops the process\n",
    "        break;\n",
    "        \n",
    "        \n",
    "# break the loop        \n",
    "cap.release()   \n",
    "vids.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebfa9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform YOLO v5 and a custom dataset using TensorFlow:\n",
    "# 1.Go to Google Colab and create a new notebook.\n",
    "# https://colab.research.google.com/ \n",
    "# 2.Change the Runtime type to GPU as a hardware accelerator by following the steps.\n",
    "# Click Runtime → Change Runtime type → select GPU in hardware accelerator dropdown → click Save.\n",
    "# 3.Clone the yolov5 repository by using the following code.\n",
    "# ! git clone https://github.com/ultralytics/yolov5.git\n",
    "# 4.Install YOLO v5 Dependencies in the Colab notebook.\n",
    "\n",
    "! pip install -U -r requirements.txt\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from iPython.display import Image, clear_output\n",
    "# from utils.google_utils import gdrive_download\n",
    "\n",
    "clear_output()\n",
    "\n",
    "%cd /content/yolov5/\n",
    "\n",
    "# 5.Download the custom (fish images) data set that you are interested in (using the Roboflow api).\n",
    "\n",
    "# https://public.roboflow.com/\n",
    "\n",
    "!curl -L \"https://public.roboflow.com/ds/lujgbDXgkE?key=CgA5u2f1oB\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip\n",
    "\n",
    "\n",
    "# Create the custom model configuration file\n",
    "\n",
    "#extracting information from the roboflow file\n",
    "%cat /content/yolov5/data.yaml\n",
    "\n",
    "\n",
    "# 6.Define the number of classes based on data.yaml.\n",
    "\n",
    "import yaml\n",
    "with open(\"data.yaml\", 'r') as stream:\n",
    "    num_classes = str(yaml.safe_load(stream)['nc'])\n",
    "\n",
    "%cat /content/yolov5/models/yolov5s.yaml\n",
    "\n",
    "\n",
    "# 7.Customize iPython write file so we can write variables.\n",
    "\n",
    "from IPython.core.magic import register_line_cell_magic\n",
    "\n",
    "@register_line_cell_magic\n",
    "def writetemplate(line, cell):\n",
    "    with open(line, 'w') as f:\n",
    "        f.write(cell.format(**globals()))\n",
    "\n",
    "%%writetemplate /content/yolov5/models/custom_yolov5s.yaml\n",
    "\n",
    "# parameters\n",
    "nc: {num_classes}  # number of classes\n",
    "depth_multiple: 0.33  # model depth multiple\n",
    "width_multiple: 0.50  # layer channel multiple\n",
    "\n",
    "# anchors\n",
    "anchors:\n",
    "  - [10,13, 16,30, 33,23]  # P3/8\n",
    "  - [30,61, 62,45, 59,119]  # P4/16\n",
    "  - [116,90, 156,198, 373,326]  # P5/32\n",
    "\n",
    "# YOLOv5 backbone\n",
    "backbone:\n",
    "  # [from, number, module, args]\n",
    "  [[-1, 1, Focus, [64, 3]],  # 0-P1/2\n",
    "   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n",
    "   [-1, 3, BottleneckCSP, [128]],\n",
    "   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n",
    "   [-1, 9, BottleneckCSP, [256]],\n",
    "   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n",
    "   [-1, 9, BottleneckCSP, [512]],\n",
    "   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n",
    "   [-1, 1, SPP, [1024, [5, 9, 13]]],\n",
    "   [-1, 3, BottleneckCSP, [1024, False]],  # 9\n",
    "  ]\n",
    "\n",
    "# YOLOv5 head\n",
    "head:\n",
    "  [[-1, 1, Conv, [512, 1, 1]],\n",
    "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
    "   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n",
    "   [-1, 3, BottleneckCSP, [512, False]],  # 13\n",
    "\n",
    "   [-1, 1, Conv, [256, 1, 1]],\n",
    "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
    "   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n",
    "   [-1, 3, BottleneckCSP, [256, False]],  # 17 (P3/8-small)\n",
    "\n",
    "   [-1, 1, Conv, [256, 3, 2]],\n",
    "   [[-1, 14], 1, Concat, [1]],  # cat head P4\n",
    "   [-1, 3, BottleneckCSP, [512, False]],  # 20 (P4/16-medium)\n",
    "\n",
    "   [-1, 1, Conv, [512, 3, 2]],\n",
    "   [[-1, 10], 1, Concat, [1]],  # cat head P5\n",
    "   [-1, 3, BottleneckCSP, [1024, False]],  # 23 (P5/32-large)\n",
    "\n",
    "   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n",
    "  ]\n",
    "\n",
    "\n",
    "\n",
    "# 8.Train yolov5 on the custom images using the custom configuration file.\n",
    "\n",
    "# train yolov5s on custom data for 100 epochs\n",
    "# time its performance\n",
    "%%time\n",
    "%cd /content/yolov5/\n",
    "!python train.py --img 416 --batch 16 --epochs 100 --data ./data.yaml --cfg ./models/custom_yolov5s.yaml --weights '' --name yolov5s_results  --cache\n",
    "\n",
    "# 9.Run yolov5 detection on images.\n",
    "\n",
    "# run yolov5 detection on images.\n",
    "\n",
    "# copy the location of the weights file and replace it in the following code \n",
    "\n",
    "!python detect.py --weights /content/yolov5/runs/train/yolov5s_results/weights/best.pt --img 416 --conf 0.4 --source ./test/images\n",
    "\n",
    "\n",
    "import glob\n",
    "from IPython.display import Image, display\n",
    "\n",
    "for imageName in glob.glob('/content/yolov5/runs/detect/exp3/*.jpg'):\n",
    "    display(Image(filename=imageName))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    \n",
    "# copy the location of the weights file and replace it in the following code \n",
    "\n",
    "!python detect.py --weights runs/train/yolov5s_results4/weights/best.pt --img 416 --conf 0.5 --source ../aquarium.mp4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f9c332",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Refer to the following code for the output of the face recognition code.\n",
    "\n",
    "from architecture import * \n",
    "import os \n",
    "import cv2\n",
    "import mtcnn\n",
    "import pickle \n",
    "import numpy as np \n",
    "from sklearn.preprocessing import Normalizer\n",
    "from tensorflow.models import load_model\n",
    "\n",
    "\n",
    "\n",
    "face_data = 'Faces/'\n",
    "required_shape = (160,160)\n",
    "face_encoder = InceptionResNetV2()\n",
    "path = \"/Face recognition/facenet_keras.h5\"\n",
    "face_encoder.load_weights(path)\n",
    "face_detector = mtcnn.MTCNN()\n",
    "encodes = []\n",
    "encoding_dict = dict()\n",
    "l2_normalizer = Normalizer('l2')\n",
    "\n",
    "import cv2 \n",
    "import numpy as np\n",
    "import mtcnn\n",
    "from architecture import *\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from scipy.spatial.distance import cosine\n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "# 7.Train the dataset using the train.py file and test the code using the face_reg.py file using the following code.\n",
    "l2_normalizer = Normalizer('l2')\n",
    "\n",
    "def normalize(img):\n",
    "    mean, std = img.mean(), img.std()\n",
    "    return (img - mean) / std\n",
    "\n",
    "confidence_t=0.99\n",
    "recognition_t=0.5\n",
    "required_size = (160,160)\n",
    "\n",
    "def get_face(img, box):\n",
    "    x1, y1, width, height = box\n",
    "    x1, y1 = abs(x1), abs(y1)\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "    face = img[y1:y2, x1:x2]\n",
    "    return face, (x1, y1), (x2, y2)\n",
    "\n",
    "def get_encode(face_encoder, face, size):\n",
    "    face = normalize(face)\n",
    "    face = cv2.resize(face, size)\n",
    "    encode = face_encoder.predict(np.expand_dims(face, axis=0))[0]\n",
    "    return encode\n",
    "\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        encoding_dict = pickle.load(f)\n",
    "    return encoding_dict\n",
    "\n",
    "def detect(img ,detector,encoder,encoding_dict):\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = detector.detect_faces(img_rgb)\n",
    "    for res in results:\n",
    "        if res['confidence'] < confidence_t:\n",
    "            continue\n",
    "        face, pt_1, pt_2 = get_face(img_rgb, res['box'])\n",
    "        encode = get_encode(encoder, face, required_size)\n",
    "        encode = l2_normalizer.transform(encode.reshape(1, -1))[0]\n",
    "        name = 'unknown'\n",
    "\n",
    "        distance = float(\"inf\")\n",
    "        for db_name, db_encode in encoding_dict.items():\n",
    "            dist = cosine(db_encode, encode)\n",
    "            if dist < recognition_t and dist < distance:\n",
    "                name = db_name\n",
    "                distance = dist\n",
    "\n",
    "        if name == 'unknown':\n",
    "            cv2.rectangle(img, pt_1, pt_2, (0, 0, 255), 2)\n",
    "            cv2.putText(img, name, pt_1, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 1)\n",
    "        else:\n",
    "            cv2.rectangle(img, pt_1, pt_2, (0, 255, 0), 2)\n",
    "            cv2.putText(img, name , (pt_1[0], pt_1[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                        (0, 0, 255), 2)\n",
    "    return img \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    required_shape = (160,160)\n",
    "    face_encoder = InceptionResNetV2()\n",
    "    path_m = \"facenet_keras.h5\"\n",
    "    face_encoder.load_weights(path_m)\n",
    "    encodings_path = 'encodings.pkl'\n",
    "    face_detector = mtcnn.MTCNN()\n",
    "    encoding_dict = load_pickle(encodings_path)\n",
    "    \n",
    "    cap = cv2.VideoCapture(\"videoplayback.mp4\")\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret,frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            print(\"CAM NOT OPEND\") \n",
    "            break\n",
    "        \n",
    "        frame= detect(frame , face_detector , face_encoder , encoding_dict)\n",
    "\n",
    "        cv2.imshow(\"frame\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7e3c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Real-time age prediction using TensorFlow with images and OpenCV:\n",
    "\n",
    "# 1.Download the “age-model.h5” files from the following website. \n",
    "# https://www.kaggle.com/datasets/mugeshraja/real-time-age-prediction\n",
    "# 2.Load the “age-model.h5” files and run the following code and load the  ‘haarcascade_frontalface_default.xml’ file. \n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the pre-trained ResNet-50 model\n",
    "model = load_model('age-model.h5')\n",
    "# Load the Haar Cascade classifier for face detection\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "\n",
    "def predict_age(image):\n",
    "    # Preprocess the image\n",
    "    image = cv2.imread(image)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "\n",
    "    # Use the fully connected layer to predict the age of the person in the image\n",
    "    age =round(model.predict(image)[0][0])\n",
    "    return age\n",
    "\n",
    "\n",
    "predict_age(\"image.jpg\")\n",
    "\n",
    "\n",
    "\n",
    "# Here's an example of how to perform Real-time age prediction using TensorFlow with videos and OpenCV:\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the pre-trained ResNet-50 model\n",
    "model = load_model('age-model.h5')\n",
    "# Load the Haar Cascade classifier for face detection\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Define a function to predict the age of a person from an image\n",
    "def predict_age(image):\n",
    "    # Preprocess the image\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "\n",
    "\n",
    "    # Use the ResNet-50 model to extract features from the image\n",
    "    age =round(model.predict(image)[0][0])\n",
    "   \n",
    "\n",
    "    return age\n",
    "\n",
    "# Define a function to capture video from the default camera and predict the age of the person in each frame\n",
    "def real_time_age_prediction():\n",
    "    cap = cv2.VideoCapture(\"videoplayback.mp4\")\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Predict the age of the person in the frame\n",
    "        age = predict_age(frame)\n",
    "\n",
    "        # Display the age on the frame\n",
    "        cv2.putText(frame, str(age), (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "        \n",
    "        # Convert the image to grayscale for face detection\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect faces in the grayscale image using the Haar Cascade classifier\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "        # Draw rectangles around the detected faces\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('frame', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the capture and destroy the window\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Call the function for real-time age prediction\n",
    "real_time_age_prediction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6b4d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform Real-time age prediction using TensorFlow with OpenCV:\n",
    "# 1.Download the emotion dataset files from the following website. \n",
    "# https://www.kaggle.com/datasets/jonathanoheix/face-expression-recognition-dataset\n",
    "\n",
    "    \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten,BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D\n",
    "import os\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "## assign the image size and class\n",
    "num_classes = 5  # ['Angry','Happy','Neutral','Sad','Surprise'] \n",
    "img_rows,img_cols = 48,48\n",
    "batch_size = 32\n",
    "\n",
    "## give the path to training and validation datasets\n",
    "train_data_dir = 'Emotion Dataset\\\\fer2013\\\\train'\n",
    "validation_data_dir = 'Emotion Dataset\\\\fer2013\\\\validation'\n",
    "\n",
    "## split the data\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,rotation_range=30,shear_range=0.3,\n",
    "                                   zoom_range=0.3,width_shift_range=0.4,\n",
    "                                   height_shift_range=0.4,horizontal_flip=True,fill_mode='nearest')\n",
    "     \n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "     \n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_data_dir,color_mode='grayscale',\n",
    "                                                    target_size=(img_rows,img_cols),batch_size=batch_size,\n",
    "                                                    class_mode='categorical',shuffle=True)\n",
    "     \n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(validation_data_dir,color_mode='grayscale',\n",
    "                                                    target_size=(img_rows,img_cols),batch_size=batch_size,\n",
    "                                                    class_mode='categorical',shuffle=True)\n",
    "\n",
    "## build the model\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',input_shape=(img_rows,img_cols,1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',input_shape=(img_rows,img_cols,1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "     \n",
    "\n",
    "model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "     \n",
    "\n",
    "model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "     \n",
    "\n",
    "model.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "     \n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "     \n",
    "\n",
    "model.add(Dense(64,kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "     \n",
    "\n",
    "model.add(Dense(num_classes,kernel_initializer='he_normal'))\n",
    "model.add(Activation('softmax'))\n",
    "     \n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "\n",
    "## Save the Model\n",
    "checkpoint = ModelCheckpoint('Emotion_model.h5',\n",
    "                             monitor='val_loss',\n",
    "                             mode='min',\n",
    "                             save_best_only=True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss',\n",
    "                          min_delta=0,\n",
    "                          patience=3,\n",
    "                          verbose=1,\n",
    "                          restore_best_weights=True\n",
    "                          )\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2,\n",
    "                              patience=3,\n",
    "                              verbose=1,\n",
    "                              min_delta=0.0001)\n",
    "     \n",
    "\n",
    "callbacks = [earlystop,checkpoint,reduce_lr]\n",
    "\n",
    "## Compile the CNN Model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = Adam(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "     \n",
    "\n",
    "nb_train_samples = 24176\n",
    "nb_validation_samples = 3006\n",
    "epochs=25\n",
    "     \n",
    "\n",
    "history=model.fit_generator(\n",
    "                train_generator,\n",
    "                steps_per_epoch=nb_train_samples//batch_size,\n",
    "                epochs=epochs,\n",
    "                callbacks=callbacks,\n",
    "                validation_data=validation_generator,\n",
    "                validation_steps=nb_validation_samples//batch_size)\n",
    "\n",
    "## Plot the Train and Validation Accuracy_Loss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "     \n",
    "\n",
    "def plot_learningCurve(history):\n",
    "  # Plot training & validation accuracy values\n",
    "    epoch_range = range(1, 12)\n",
    "    plt.plot(epoch_range, history.history['accuracy'])\n",
    "    plt.plot(epoch_range, history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "  # Plot training & validation loss values\n",
    "    plt.plot(epoch_range, history.history['loss'])\n",
    "    plt.plot(epoch_range, history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper left')\n",
    "    plt.show()\n",
    "     \n",
    "\n",
    "plot_learningCurve(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58bf7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download the ‘haarcascade_frontalface_default.xml’ file and test the emotion model using the following code.\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "print(cv2.__version__)\n",
    "\n",
    "import datetime\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing import image\n",
    "face_classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "classifier =load_model('Emotion_model.h5')\n",
    "\n",
    "class_labels = ['Angry','Happy','Neutral','Sad','Surprise']\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0);\n",
    "\n",
    "print(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "print(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Define the codec and create VideoWriter object \n",
    "fourcc =cv2.VideoWriter_fourcc(*'MJPG')      \n",
    "out =cv2.VideoWriter('output.avi', fourcc, 20.0 ,(640, 480))\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = cap.read()\n",
    "    labels = []\n",
    "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray,1.3,5)\n",
    "    \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        \n",
    "    text = 'Width:' + str(cap.get(3)) + \"Height:\" + str(cap.get(4))\n",
    "    datet = str(datetime.datetime.now())\n",
    "\n",
    "    txt = cv2.putText(frame , datet, (10, 50),font, 1 , (0,255,255) , 2, cv2.LINE_AA)\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h,x:x+w]\n",
    "        roi_gray = cv2.resize(roi_gray,(48,48),interpolation=cv2.INTER_AREA)\n",
    "    # rect,face,image = face_detector(frame)\n",
    "\n",
    "\n",
    "        if np.sum([roi_gray])!=0:\n",
    "            roi = roi_gray.astype('float')/255.0\n",
    "            roi = img_to_array(roi)\n",
    "            roi = np.expand_dims(roi,axis=0)\n",
    "\n",
    "        # make a prediction on the ROI, then lookup the class\n",
    "\n",
    "            preds = classifier.predict(roi)[0]\n",
    "            label=class_labels[preds.argmax()]\n",
    "            label_position = (x,y)\n",
    "            cv2.putText(frame,label,label_position,font,2,(0,255,0),3)\n",
    "        else:\n",
    "            cv2.putText(frame,'No Face Found',(20,60),font,2,(0,255,0),3)\n",
    "            \n",
    "    cv2.imshow('Emotion Detector',frame)\n",
    "    \n",
    "    out.write(frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2917357",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run the following code to perform CBIR. \n",
    "from zipfile import ZipFile\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from glob import glob\n",
    "import cv2, os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.preprocessing import image\n",
    "\n",
    "\n",
    "6.\tUnzip the file.\n",
    "file_name = \"/dataset.zip\"\n",
    "\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "    zip.extractall()\n",
    "    print('done')\n",
    "\n",
    "\n",
    "# display the image\n",
    "img = cv2.imread('/dataset/1001.jpg', -1)\n",
    "from google.colab.patches import cv2_imshow\n",
    "cv2_imshow(img)\n",
    "\n",
    "\n",
    "## Load the images.\n",
    "path = '/dataset'\n",
    "\n",
    "img_dataset = []\n",
    "def load_img():\n",
    "    img_path = os.path.join(path, \"*\")\n",
    "    for im in glob(img_path):\n",
    "        img  = cv2.imread(im)\n",
    "        data = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        data = cv2.resize(data, (512, 512))\n",
    "        data = image.img_to_array(data)\n",
    "        img_dataset.append(data)\n",
    "    print('All images loaded')\n",
    "load_img()\n",
    "# All images loaded\n",
    "print('Number of the Images: {}'.format(len(img_dataset)))\n",
    "\n",
    "\n",
    "## display the random images \n",
    "import random\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    img = image.array_to_img(random.choice(img_dataset))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "### Split the data.\n",
    "end =round(len(img_dataset) * 0.95)\n",
    "\n",
    "X_train =img_dataset[:end]\n",
    "X_test  =img_dataset[end:]\n",
    "print(len(X_train) ,len(X_test))\n",
    "\n",
    "## Normalize the data\n",
    "X_train = np.asarray(X_train) / 255\n",
    "X_test = np.asarray(X_test) / 255\n",
    "\n",
    "## Reshape the data to have 1 channel\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "X_train = np.reshape(X_train, (-1, 512, 512, 1))\n",
    "X_test = np.reshape(X_test, (-1, 512, 512, 1))\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "### Create the autoencoder.\n",
    "input_img = Input(shape=(512,512,1))\n",
    "x = Conv2D(32,(3,3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2,2), padding='same')(x)\n",
    "x = Conv2D(16,(3,3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2,2), padding='same')(x)\n",
    "x = Conv2D(8,(3,3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2,2), padding='same')(x)\n",
    "x = Conv2D(4,(3,3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2,2), padding='same')(x)\n",
    "x = Conv2D(2,(3,3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2,2), padding='same', name='encoder')(x)\n",
    "\n",
    "x = Conv2D(2, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(4, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu',  padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.summary()\n",
    "\n",
    "\n",
    "### Train and save the model.\n",
    "autoencoder.fit(X_train, X_train, epochs=4, batch_size=16, callbacks=None );\n",
    "autoencoder.save('autoencoder.h5')\n",
    "\n",
    "# Create the encoder part\n",
    "# The encoder part is the first half of the autoencoder, i.e. the part that will encode the input into a latent space representation. In this case, the dimension of this representation is \n",
    "\n",
    "autoencoder =Model('/content/autoencoder.h5')\n",
    "encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('encoder').output)\n",
    "encoder.save('encoder.h5')\n",
    "encoder =Model('/content/encoder.h5')\n",
    "\n",
    "# Load the query image\n",
    "# We take a query image from the test set\n",
    "\n",
    "query = X_test[8]\n",
    "plt.imshow(query.reshape(512,512), cmap='gray');\n",
    "\n",
    "# Encode the test images and the query image\n",
    "X_test.shape\n",
    "# We remove the query image from the test set (the set in which we will search for close images)\n",
    "\n",
    "X_test = np.delete(X_test, 8, axis=0)\n",
    "X_test.shape\n",
    "\n",
    "\n",
    "# Encode the query image and the test set\n",
    "codes = encoder.predict(X_test)\n",
    "query_code = encoder.predict(query.reshape(1,512,512,1))\n",
    "codes.shape\n",
    "\n",
    "query_code.shape\n",
    "\n",
    "a=1\n",
    "for i in query_code[0].shape:\n",
    "    a = a*i\n",
    "print(a)\n",
    "\n",
    "\n",
    "### Find the closest images.\n",
    "# We will find the 9 closest images\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "n_neigh = 9\n",
    "codes = codes.reshape(-1, a); print(codes.shape)\n",
    "query_code = query_code.reshape(1, a); print(query_code.shape)\n",
    "\n",
    "# Fit the KNN to the test set\n",
    "nbrs = NearestNeighbors(n_neighbors=n_neigh).fit(codes)\n",
    "distances, indices = nbrs.kneighbors(np.array(query_code))\n",
    "closest_images = X_test[indices]\n",
    "closest_images = closest_images.reshape(-1,512,512,1); print(closest_images.shape)\n",
    "\n",
    "\n",
    "### Get the closest images.\n",
    "plt.imshow(query.reshape(512,512), cmap='gray');\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "for i in range(n_neigh):\n",
    "    # display original\n",
    "    ax = plt.subplot(1, n_neigh, i+1)\n",
    "    plt.imshow(closest_images[i].reshape(512, 512))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dbbc52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a2b3cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0190ef13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0744feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a1c6b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e959ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540816a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5ac74d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67aba45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36562c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
